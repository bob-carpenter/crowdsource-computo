---
title: "Crowdsourcing with Difficulty"
subtitle: "A Bayesian Rating Model for Heterogeneous Items"
author:
  - name: Bob Carpenter
    corresponding: true
    email: bcarpenter@flatironinstitute.org
    url: https://bob-carpenter.github.io/
    orcid: 0000-0002-2433-9688
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
date: last-modified
date-modified: last-modified
description: |
abstract: >+
  Dawid and Skene's crowdsourcing model fails to capture important distributional properties of real rating data.  By adding effects for difficulty, discriminativeness, and guessability, the annotation model is able to more closely model the true data generating process.  This is illustrated with binary data from dental image classification and natural language inference where rater response may be characterized through sensitivity and specificity.
keywords: [rating model, Dawid-Skene, item-response theory, Bayesian, Stan, Python]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: bob-carpenter
repo: "crowdsource-computo"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
---

# Introduction

This note considers the rating problem for classification.  An example analyzed later is a data set of roughly 4000 dental X-rays, each of which is rated by 5 dentists as to whether the patient had caries (a kind of pre-cavity).  The dentists showed surprisingly little agreement, with only 100 X-rays having consensus of caries being present.  The dentists also varied dramatically in the number of cases each rated as having caries.  There are several goals for a rating mode, wuch as combining ratings into a consensus analysis, analyzing the accuracy and bias of the individual raters, analyzing the difficulty of the rating task, and analyzing the prevalence of caries among the population represented by the X-rays.  The results may also be used to guide active learning in data acquisition by assessing the information gain from gathering a rating from either a known or a new rater.

One of the main applications is in crowdsourcing, where typically untrained or lightly trained workers perform bulk ratings of highly variable quality.  But these models are equally useful when applied to expert ratings, where expertise is measured in terms of rating accuracy and bias.

Rating models show up in multiple fields from educational testing, from where we will draw much of the inspiration for the contribution here, in epidemiology where they are applied to diagnostic tests, in sociology where they are applied to exploring cultural norms, and in data curation for machine learning where they are applied to human feedback on classification or other problems.  As one example, human tags on images are used to train large-scale image classifiers like ImageNet, which has over a million images classified into a thousand classes.  As another example, crowdsourced ratings of language model output are used as a fine-tuning step that adjusts a foundational large language model like GPT-4, which is trained to complete text, into a chatbot like ChatGPT that is (imperfectly) trained to be helpful, truthful, and harmless.


# A General Crowdsourcing Model

This section presents the most general crowdsourcing model, then considers how it may be simplified by removing features.  Removing features here amounts to tying parameters, such as assuming all raters have the same accuracy or all items have the same difficulty.

## The rating data

Consider a crowdsourcing problem for which there are $I \in \mathbb{N}$ items to rate and $J \in \mathbb{N}$ raters to do the rating.  Long-form data will be used to accomodate there being a varying number of raters per item and a varying number of items per rater.  let $N \in \mathbb{N}$ be the number of ratings $\textrm{rating}_n \in \{0, 1 \}$, each of which is equipped with a rater $\textrm{rater}_n \in 1:J$ and item being rated $\textrm{item}_n \in 1:I$.

For each item $i$, suppose there are covariates $x_i \in \mathbb{R}^K$.  Then $x \in \mathbb{R}^{I \times K}$ is the full data matrix and $x_i$ is naturally considered a row vector.  In the simplest case, $K = 1$ and $x$ is a single column of 1 values representing an intercept.  In the more general case, other features may denote information relevant for classifying the item, such as the pixels of an image for image classification, demographic information for predicting a vote in an election, or medical test results for predicting a medical condition.

## The generative model

The generative model derives from combining Dawid and Skene's epidemiology model with sensitivity and specificity with the item-response theory educational testing model with item difficulty, disciminativeness, and guessing. The categories for the items are generated independently given the covariates using a logistic regression.  The ratings for an item are then generated conditionally based on the item's category and the rater's abilities and biases.  

In frequentist terms, this section presents a complete data likelihood for the categories and rating data.  The categories are not observed and are thus treated as missing data to allow them to be marginalized to derive the likelihood for the rating data.

### Generating categories

For each item, let $z_i \in \{ 0, 1 \}$ be its (unobserved/latent) category, with a 1 conventionally denoting "success" or a "positive" result.  The 

The complete data likelihood is complete in the sense that it includes the latent category.  Marginalizing out this category, the technical details and numerical stability of which are deferred until later, leads to the ordinary likelihood function used in the model to avoid challenging inference over discrete parameters.

Let $\beta \in \mathbb{R}^K$ be a vector of (unobserved) regression coefficients.  Let $\pi \in (0, 1)$ be the parameter representing the prevalence of positive outcomes.  

Categories are generated independently given the prevalence,
\begin{equation}
z_i \sim \textrm{bernoulli}(\pi).
\end{equation}

### Generating ratings

The rating from rater $j$ for item $i$ is generated conditionally given the category $z_i$ of an item.  For positive items $z_i = 1$), sensitivity (i.e., accuracy on positive items) is used, whereas for negative items ($z_i = 0$), specificity (i.e., accuracy on negative items) is used.  Thus every rater $j$ will have a sensitivity $\alpha^{\textrm{sens}}_j \in \mathbb{R}$ and a specificity $\alpha^{\textrm{spec}}_j$ on the log odds scale.  If the sensitivity is higher than the specificity there will be a bias toward 1 ratings, whereas if the specificity is higher than the sensitivity, there is a bias toward 0 ratings.  If the model only has sensitivity and specificity parameters that vary by rater, it reduces to Dawid and Skene's diagnostic testing model.  Fixing $\alpha^{\textrm{sens}} = \alpha^{\textrm{spec}}$ introduces an unbiasedness assumption whereby a rater has equal sensitivities and specificities.

The items are parameterized with a difficulty $\beta_i \in \mathbb{R}$ on the log odds scale.  This difficulty is subtracted form the sensitivity (if $z_i = 1$) or specificity (if $z_i = 0$) as appropriate to give the raw log odds of a correct rating (i.e., a rating matching the true category $z_i$).  Fixing $\beta_i = 0$ introduces the assumption that every item is equally difficult.

Each item is further parameterized with a positive-constrained discrimination parameter $\delta_i \in (0, \infty)$.  This is multiplied by the raw log odds to give a discrimation-adjusted log odds to give a probability of correctly rating the item.  With high discrimination, it is more likely a rater with ability greater than the difficulty will get the correct answer and less likely that a rater with ability less than difficulty will get the correct answer.  For educational testing, high discrimation test questions are preferable, but for rating wild type data, low discrimination items are common because of natural variations in the signal (e.g., natural language text or an image).  Fixing $\delta_i = 1$ introduces the assumption that the items are equally discriminative.

The final parameter associated with an item is a guessability parameter $\lambda_i \in (0, 1)$, giving the probability that a rater can just "guess" the right answer.  The probability that a rater assigns the correct rating will thus be the combination of the probability of guessing correctly and otherwise getting the correct answer in the usual way.  Fixing $\lambda_i = 0$ introduces the assumption that the raters never guess an answer.  

Without a guessing parameter, as difficulty goes to infinity, the probability a rater provides the correct label for an item goes to zero.  With guessing, the probability of a correct label is always at least the probability of guessing.

The full model follows the item-response theory three-parameter logistic (IRT-3PL) model, where the probability that rater $j$ assigns the correct rating to item $i$ is given by
\begin{equation}
y \sim \textrm{binomial}\!\left(\lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot (\alpha^k_j - \beta_i))\right),
\end{equation}
where $k = \textrm{sens}$ if $z_i = 1$ and $k = \textrm{spec}$ if $z_i = 0.$

In order to convert to a distribution over rating, the probability of a 1 outcome must be flipped when $z_i = 0$ so that a 90\% accurate rating results in a 90% chance of a 0 rating,
\begin{equation}
y \sim
\begin{cases}
\textrm{binomial}\!\left(\lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}\!\left(\delta_i \cdot \left(\alpha^\textrm{sens}_j - \beta_i\right)\right)\right)
& \textrm{if } z_i = 1, \textrm{ and}
\\[4pt]
\textrm{binomial}\!\left(1 - \left( \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}\!\left(\delta_i \cdot \left(\alpha^\textrm{sens}_j - \beta_i\right)\right)\right)\right)
& \textrm{if } z_i = 0.
\end{cases}
\end{equation}
The second case ($z_i = 0$) reduces to
\begin{equation}
\textrm{binomial}\!\left( \left( 1 - \lambda_i \right)
                \cdot \left( 1 - \textrm{logit}^{-1}\!\left(\delta_i \cdot \left(\alpha^\textrm{sens}_j - \beta_i\right)\right)\right)\right).
\end{equation}

### Adding predictors

If covariates are available that inform the parameters, they may be incorporated in a very general way through a generalized linear model.

Suppose there are item-level covariates $x_i \in \mathbb{R}^K$.  With a parameter vector $\gamma \in \mathbb{R}^K$, the generative model for the category of an item may be extended to a logistic regression,
\begin{equation}
z_i \sim \textrm{bernoulli}\!\left( x_i \cdot \beta \right).
\end{equation}
Here, $x_i \in \mathbb{R}^K$ is considered a row vector because it is a row of the data matrix $x \in \mathbb{I \times K}$.  An intercept for the regression may be added by including a column of 1 values in $x$.  The resulting coefficients will act as a classifier for new items by generating a covariate-specific prevalence.  This motivated @raykar2010's combination of Dawid and Skene's rating model and a logistic regression of this form. 

Item-level coariates can be used to generate the item-level parameters of difficulty, discrimination, and guessability.  For example, a covariate that indicates the number of options in a multiple choice test would inform the guessability parameter.  If the grade level of the textbook from which a problem was culled was included as a predictor, that could be used to inform the difficulty parameter.

If there are rater level covariates $u_i \in \mathbb{R}^L$, these may be used in the same way to generate the ability parameters for the raters.  For example, there might be an indicator of whether the rater was an undergraduate, graduate student, faculty member, or crowdsourced external agent, which could be used to inform ability.


## Marginal likelihood

The generative model leaves us with latent discrete categories $z_i \in \{ 0, 1 \}$ for each item.  For both optimization and sampling, it is convenient to marginalize the complete likelihood $p(y, z \mid \pi, \alpha, \beta, \delta, \lambda)$ to the rating likelihood $p(y \mid \pi, \alpha, \beta, \delta, \lambda)$.  This notation suppresses the conditioning throughout on the design variables $\textrm{rater}$ and $\textrm{item}$.  The marginalization calculation is efficient because it is factored by data item.  Letting $\theta = \pi, \alpha, \beta, \delta, \lambda$ be the full set of continuous parameters, the trick is to rearrange the long-form data by item, by selecting the $n$ by item (i.e., by $\textrm{item}_n$).
\begin{equation}
\begin{array}{rcl}
p(y, z \mid \theta)
& = &
\prod_{i=1}^I p(z_i \mid \theta)
\cdot
\prod_{n = 1}^N p(y_n \mid z, \theta)
\\[4pt]
& = & 
\prod_{i=i}^I
  \left( p(z_i \mid \theta)
         \cdot
	 \prod_{n : \textrm{item}_n = i} p(y_n \mid z_i, \theta)
  \right).
\end{array}
\end{equation}

On a per item basis, the marginalization is tractable,
\begin{equation}
p(y \mid \theta)
= \prod_{i=i}^I
  \sum_{z_i = 0}^1 \,
          p(z_i \mid \theta)
                \cdot
		\prod_{n : \textrm{item}_n = i} p(y_n \mid z_i, \theta).
\end{equation}

Computational inference requires a log likelihood.  The log marginal likelihood of the rating data is
\begin{equation}
\begin{array}{rcl}
\log p(y \mid \theta)
& = & \log \prod_{i=i}^I
  \sum_{z_i = 0}^1 \,
          p(z_i \mid \theta)
                \cdot
		\prod_{n : \textrm{item}_n = i} p(y_n \mid z_i, \theta).
\\[4pt]
& = &
\sum_{i=1}^I
\textrm{logSumExp}_{z_i = 0}^1 \,
  \left(
    \log p(z_i \mid \theta)
    + 
    \sum_{n : \textrm{item}_n = i} \log p(y_n \mid z_i, \theta)
  \right),
\end{array}
\end{equation}
where
\begin{equation}
\textrm{logSumExp}_{n = 1}^N \, \ell_n
= \log \sum_{n=1}^N \exp(\ell_n)
\end{equation}
is the numerically stable log-scale analogue of addition.

## Model reductions

By tying or fixing parameters, the full model may be reduced to define a wide range of natural submodels.  Six of these models correspond to item-response theory models of the one-, two-, and three-parameter logistic variety, either with or without a sensitivity/specificity distinction. The model with varying rater sensitivity and specificity and no item effects reduces to Dawid and Skene's model.  Other models, such as the model with a single item effect and no rater effects have been studied in the epidemiology literature.

The table below summarizes the possible model reductions and gives them tags by which they can be abbreviated when evaluating models.

| Reduction | Description | Tag |
|:---------:|:-----------:|:---:|
| $\lambda_i = 0$ |  no guessing items | A |
| $\delta_i = 1$ | equal discrimination items | B |
| $\beta_i = 0$ |equal difficulty items | C |
| $\alpha^{\textrm{spec}} = \alpha^{\textrm{sens}}$ | unbiased raters | D |
| $\alpha_j = \alpha_j'$ | identical raters | E |


The first list contains models that model bias by splitting accuracy into sensitivity and specificity components.  Models without a sensitivity versus specificity distinction make sense when the categories are not ordered.  For example, asking consumers which brand is their favorite between two brands at a time, the labels 0 and 1 are arbitrary, and sensitivity should be equal to specificity.  The following models do not distinguish sensitivity and specificity.

\begin{equation}
\begin{array}{ccl}
\textrm{Reductions} & \textrm{Probability Correct} & \textrm{Note} \\ \hline
D
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot (\alpha_j - \beta_i)) & \textrm{\small IRT 3PL}
\\ 
CD
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot \alpha_j)
\\
BD
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha_j - \beta_i) & \textrm{\small IRT 2PL}
\\ \hline
BCD
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha_j)
\\
AD
& \textrm{logit}^{-1}(\delta_i \cdot (\alpha_j - \beta_i))
\\
ACD
& \textrm{logit}^{-1}(\delta_i \cdot \alpha_j)
\\ \hline
ABD
& \textrm{logit}^{-1}(\alpha_j - \beta_i) & \textrm{\small IRT 1PL}
\\
ABCD
& \textrm{logit}^{-1}(\alpha_j)
\\
ABCDE
& \textrm{logit}^{-1}(\alpha)
\end{array}
\end{equation}
The final model in the list is the only model that does not distinguish among the raters, using a single accuracy parameter. The following models introduce separate parameters for sensitivity and specifity rather than assuming they are the same.
\begin{equation}
\begin{array}{ccl}
\textrm{Reductions} & \textrm{Probability Correct} & \textrm{Note} \\ \hline
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot (\alpha^k_j - \beta_i)) & \textrm{\small IRT 3PL + sens/spec}
\\
C
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot \alpha^k_j)
\\
BC
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha^k_j)
\\ \hline
A
& \textrm{logit}^{-1}(\delta_i \cdot (\alpha^k_j - \beta_i)) & \textrm{\small IRT 2PL + sens/spec}
\\
AC
& \textrm{logit}^{-1}(\delta_i \cdot \alpha^k_j)
\\
AB
& \textrm{logit}^{-1}(\alpha^k_j - \beta_i) & \textrm{\small IRT 1PL + sens/spec}
\\ \hline
ABC
& \textrm{logit}^{-1}(\alpha^k_j) & \textrm{\small Dawid and Skene}
\\
ABCE
& \textrm{logit}^{-1}(\alpha^k)
\end{array}
\end{equation}
The final model in this list has a single sensitivity and specificity for all raters, whereas the other models have varying effects.

There is a single model that does not use any notion of rating accuracy at all, relying solely on item effects.
\begin{equation}
\begin{array}{cc}
\textrm{Reductions} & \textrm{Probability Correct} \\ \hline
ABDE
& \textrm{logit}^{-1}(- \beta_i) \quad
\\ \hline
\end{array}
\end{equation}

The remaining models are all redundant in the sense that fixing their non-identifiability issues reduces to a model with a single item effect.
\begin{equation}
\begin{array}{ccc}
\textrm{Reductions} & \textrm{Probability Correct} \\ \hline
E
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot (\alpha^k - \beta_i))
\\
DE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot (\alpha - \beta_i))
\\
CE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot \alpha^k)
\\ \hline
CDE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\delta_i \cdot \alpha)
\\
BE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha^k - \beta_i)
\\
BDE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha - \beta_i)
\\ \hline
BCE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha^k)
\\
BCDE
& \lambda_i + (1 - \lambda_i) \cdot \textrm{logit}^{-1}(\alpha)
\\
AE 
& \textrm{logit}^{-1}(\delta_i \cdot (\alpha^k - \beta_i))
\\ \hline
ADE
& \textrm{logit}^{-1}(\delta_i \cdot (\alpha - \beta_i))
\\
ACE
& \textrm{logit}^{-1}(\delta_i \cdot \alpha^k)
\\
ACDE
& \textrm{logit}^{-1}(\delta_i \cdot \alpha)
\\ \hline
ABE
& \textrm{logit}^{-1}(\alpha^k - \beta_i)
\end{array}
\end{equation}

::: {#refs}
:::